---
title: 记一次辛苦的在网关，客户端和服务端找乱码bug的问题
date: 2018-04-01 13:49:12
tags: [encode_bug]
categories: bug修正记录
---

## 乱码问题的起因和解决的过程，以及设计到的一些知识（附录有本人对字符编码和字符集的理解）

由于最近公司准备开发属于自己的网关。作为服务端的我需要和网关开发人员进行协议的制定和对接。

通信协议我们使用的是MQTT

网关人员在设备连接上线的时候会上报网关设备以及网关设备下面挂载的子设备信息，其中包括了设备的名称，有些是英文的，有些是中文的。

作为服务端的我这边接收到了对应的消息，我就会把对应的设备数据存储进mysql数据库存储起来。在这之间通信的数据的编码都是"UTF-8"。在我们调通之后，数据库里面的数据中文是正常显示的，但是在断网重连重新上报的时候竟然出现了乱码的字段。反复试了几次，乱码和不乱码的情况都会发生

客户端(网关端)和服务端编码的方式要一致

因为这段代码是一个已经离职的同事写的

```Java
byte[] payload;
new String(payload)
```

而且，乱码的问题不是必现，所以一直觉得代码没有问题，以至于问题总是得不到解决。后来仔细查找代码发现了，在字节数据解码成字符串数据的时候没有指定解码的格式，也就是和客户端统一的UTF-8，后来加上了UTF-8，问题得以解决

~~~Java
byte[] payload;
new String(payload， “UTF-8”)
~~~

仔细思考了为什么这个问题不是必现的原因，后来得出的结论是

1.如果代码本身不指定编码解码的格式，那么jvm可能会根据当前开发平台的编码格式来编码解码

2.如果代码本身不指定编码解码的格式，那么jvm可能随机选择解码的格式，所以才会导致有时候不乱码，有时候又乱码的现象。

总结:编码解码的格式一定要统一，这样才能保证客户端和服务端不会出现乱码的问题



#附录

+ 用8个可以开合的晶体管来组合成不同的状态

Ascii编码（American Standard Code for information interchange）

0x20（0010 0000）即编号从0到32的编码规定了特殊的用途，称为“控制码 ”

然后剩下的编码是编码的空格，标点符号，数字，大小写字母，一直编码到了127号

，这样计算机就可以用不同的字节来存储英语的文字了。

美帝国主义在创造计算机的时候可能没有考虑到其他国家的文字也会被用到计算机中把，也可能人家觉得其他国家的人不会用。所以没有留出多余的位置表示其他的字符。



后来，世界各地的人都在使用计算机，有些国家使用的不是英文，他们的字母里面有许多是Ascii里面没有的。为了可以在计算机中保存他们的文字。他们决定采用127号之后的空位来表示这些新的字母、符号，还加入了许多画表格时需要用到的横线，竖线，交叉等形状，一直把序号编到了最后一个状态255，从128到255这一页的字符集被称<扩展字符集>



等到我们中国人民使用计算机的时候，已经没有可以利用的字节状态来表示汉字

况且有6000多个常用汉字需要保存，中国人民是聪明的，我们取消掉了127号之后的奇异符号。

###GB2312的由来

> 规定：一个小于127的字符的意义与原来是相同的,<但是两个大于127的字符连在一起的时候，就表示一个汉字了>,前面的一个字节（称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样子我们就能够组合出大约7000多个简体汉字了，在这些编码里面，我们还把数学符号，罗马希腊的字母，日文的假名都编进去了，连载ASCII里面本来就有的数字，标点，字母都统统重新编码了两个字节长的编码，这就是常说的<全角>字符，而原来在127号一下的那些就叫做<半角>字符了，然后中国人名就把这种定义汉字的方案叫做<GB2312>。所有GB2312是对ASCII的中文扩展

### GBK的由来

>没办法，中国的汉字太多了，我们很快就发现了许多人的人名没有办法打印出来，于是不得不继续把GB2312没有用到的码位找出来用上，后来还是不够用，然后继续降低要求，不再要求低字节一定是127号后的内码，只要第一个字节（高字节）是大于127就固定表示这是一个汉字的开始,不管后面跟的失败不是扩展字符集里的内容，扩展之后的编码方案被称为GBK标准，GBK包括了GB2312的所有内容，同事又增加了近20000个新的汉字（包括繁体字）和符号，后来少数名族也要用电脑了，于是继续扩展，又新加了几千个新的少数名族的字，GBK扩展成了GB18030，中国的程序员把这种汉字的编码方案叫做DBCS（Double Byte Charecter Set 双字节字符集），在DBCS系列标准里面，最大的特点是两字节长的汉字字符和一字节厂的英文字符并存于同一套编码方案里面，为了支持中文的处理，必须要注意字符串里面每一个字节的值，如果这个值是大于127的，那么认为一个双字节字符集里面的字符出现了。

### unicode的由来

>因为当时各个国家都像中国这样搞出一套自己的编码标准，结果相互之间谁也不懂谁的编码，谁也不支持别人的编码。
>
>这时候，就出现了一个叫ISO（国际标准化组织）的国际组织决定着手解决这个问题，他们采用的方法很简单：废了所有的地区性编码，重新搞了一个包括了地球上所有文化，所有字母和符号的编码，名字全称叫做<Universal Multiple-Octet Coded Character Set>,简称UCS ，俗称“unicode”
>
>unicode开始制定的时候，计算机的存储器容量极大的发展了，空间再也不是问题了。于是ISO就直接规定必须用两个字节，也就是16位来统一表示所有的字符，对于ASCII里面的那些“半角”字符，unicode保持其原来的编码不变，只是将其长度由原来的8位扩展为16位。而其他文化和语言的字符则全部重新统一编码。由于“半角”英文符号只需要用到低8位，所以其高八位永远都是0，因此这种大气的方案在保存英文文本的时候会多浪费一倍的空间
>
>缺点:
>
>1. 如何才能区别unicode和ascii呢，计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？
>2. 我们已经知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用3到4个字节，那么英文字母前民都必然会出现2到3个字节的0，这对于存储空间来说是极大的浪费，文本文件的大小会因此大出2、3倍。
>
>解决办法：
>
>unicode英文这个原因在很长一段时间内都无法推广，知道互联网的出现，为解决unicode如何在网络上的问题，于是面向传输的众多UTF(UCS Transfer Fromat) 标准出现了，顾名思义，UTF-8就是每次8个位传输数据，UTF-16就是每次16个位，UTF-8就是在互联网上使用最广的一种unicode的实现方式，这是为传输而设计的编码，并使编码无国界

### UTF-8详解

>UTF-8最大的一个特点，就是它可以变长来编码。它可以使用1-4个字节表示一个符号，根据不同的符号而变化字节长度，当字符在ASCII码的范围时，就用一个字节表示，保留了ASCII字符一个字节的编码作为它的一部分，注意的是<unicode一个中文字符占2个字节,而UTF-8一个中文字符占3个字节>,从unicode到utf-8并不是直接的对应，而是要通过一些算法和规则来转换，具体的转换算法网上有很多，感兴趣的可以自行搜索看看



